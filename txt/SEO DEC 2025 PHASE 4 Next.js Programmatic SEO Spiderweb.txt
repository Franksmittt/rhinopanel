The Internal Spiderweb: Architecting High-Density Link Meshes for Programmatic SEO in Next.js
1. Executive Summary: The Indexation Crisis in Programmatic Scale
The paradigm of Search Engine Optimization (SEO) has undergone a fundamental phase shift. In the era of manual content creation, the primary constraint was production volume; today, with the advent of programmatic methodologies capable of generating thousands of landing pages instantaneously, the scarcity has shifted to indexation and crawl budget. The modern challenge is not creating content, but ensuring that search engine bots—specifically Googlebot—can discover, traverse, and index vast repositories of generated pages efficiently.
This report presents a technical blueprint for "The Internal Spiderweb," a proprietary architectural model designed to maximize crawl efficiency and link equity distribution within large-scale Next.js applications. Moving beyond the limitations of traditional hierarchical site structures, which often leave deep content orphaned and unranked, the Spiderweb model utilizes a mesh topology. By leveraging the specific rendering capabilities of Next.js—including Incremental Static Regeneration (ISR) and Server Components—and integrating advanced geospatial algorithms via PostGIS, this architecture ensures that every node in the network serves as a high-speed conduit to relevant peers.
The analysis focuses on the "Related" component, a dynamic logic engine that programmatically generates links based on geospatial proximity (e.g., "Plumber in") and topical adjacency (e.g., "Leak Detection in"). By trapping the crawler in a loop of high-relevance discovery, the Internal Spiderweb forces deep indexation, distributing authority from seed pages to the periphery of the programmatic graph, effectively outperforming legacy WordPress architectures through superior engineering.1
________________
2. The Theoretical Physics of Crawl Traps and Link Graphs
To understand the necessity of the Internal Spiderweb, one must first deconstruct the mathematical and theoretical underpinnings of how search engines navigate the web. A website is, in graph theory terms, a directed graph where pages are nodes and hyperlinks are edges. The efficiency of a crawler's traversal through this graph is dictated by the topology of these connections.
2.1 The Failure of Tree Structures at Scale
Traditional websites follow a tree structure (or dendrogram), rooting at the Homepage and branching out into Categories, Subcategories, and finally, Content Leaves. In a manually curated site with 500 pages, this hierarchy is functional; the maximum depth (distance from root) rarely exceeds three or four clicks.
However, in a programmatic SEO (pSEO) context involving 50,000 pages (e.g., a service page for every city in the United States), a strict tree structure becomes a liability. If a "Plumber in Round Rock, TX" page is accessible only through Home > Services > Plumbing > Texas > Travis County > Round Rock, the click depth is excessive. Search engine crawlers operate with a finite "crawl budget"—a limit on the number of pages the bot will crawl and the resources it will expend.4 As click depth increases, the probability of a crawler reaching the leaf node diminishes exponentially. Deep pages in a tree structure suffer from "PageRank decay," receiving negligible link equity from the domain's root, often resulting in them being ignored or de-indexed by Google.1
2.2 Mesh Topology and Small-World Networks
The Internal Spiderweb proposes a shift from a tree structure to a Mesh Topology. In a mesh, nodes are interconnected not just vertically (to parents) but laterally (to siblings and cousins). This mimics a "Small-World Network," a mathematical graph property where most nodes are not neighbors of one another, but the neighbors of any given node are likely to be neighbors of each other, and most nodes can be reached from every other node by a small number of hops or steps.
2.2.1 The "Trap" Mechanism
The term "trap" in this architectural context is positive. It refers to a configuration where the internal link density is so high, and the relevance of those links so potent, that a crawler entering the graph finds a near-infinite number of valid, high-quality edges to traverse. When a programmatic page links to 50 other relevant pages (e.g., nearby suburbs or related services), it effectively presents the crawler with 50 new queues. If those destination pages also link back to the origin or to other peers in the cluster, the crawler is "trapped" within a dense pocket of the domain, cycling through content and indexing it rapidly rather than bouncing back to the SERP or exiting the site.6
2.2.2 Link Equity Recirculation
Link equity flows through a graph like a fluid in a hydraulic system. In a sparse graph (tree), equity flows down and pools at the bottom or evaporates. In a dense mesh (spiderweb), equity is recirculated. A high-authority page (e.g., a city hub like "Austin") passes authority to its satellites ("Round Rock," "Pflugerville"). By having those satellites link back to the hub and to each other, the equity is conserved within the cluster, raising the semantic authority of the entire topic/location group. This is often referred to as "Topic Clustering," but in pSEO, it is "Geospatial Clustering".8
Feature
	Tree Structure (Traditional)
	Mesh Structure (Spiderweb)
	Topology
	Hierarchical / Linear
	Interconnected / Cyclic
	Click Depth
	High (Deep)
	Low (Flat)
	Link Equity
	Dissipates at leaves
	Recirculates in clusters
	Resilience
	Single point of failure (Parent)
	Multipath redundancy
	Crawl Behavior
	Linear traversal, high abandonment
	Cyclic traversal, deep indexation
	________________
3. Next.js: The Engine of Scale
Implementing a mesh topology with thousands of nodes requires a robust rendering engine. While WordPress plugins can manage basic interlinking, they often fail at the scale of pSEO due to database bottlenecks and lack of build-time optimization. Next.js, with its React-based architecture, offers distinct advantages for constructing high-performance internal link graphs.
3.1 The Rendering Dilemma: SSG vs. SSR vs. ISR
The method by which the HTML of a page is generated dictates the feasibility of the Internal Spiderweb.
3.1.1 Static Site Generation (SSG) Limitations
Standard SSG (getStaticProps executed at build time) is the gold standard for performance. However, it scales linearly. If generating the "Related" component requires a geospatial database query taking 50ms, building 100,000 pages would take nearly 1.4 hours of pure computation, excluding overhead. In practice, this leads to build timeouts on platforms like Vercel or Netlify. Furthermore, updating the link graph (e.g., adding a new service) would require a full rebuild of the entire site, which is operationally untenable.10
3.1.2 Server-Side Rendering (SSR) Latency
SSR (getServerSideProps) generates the page on every request. While this solves the build time issue, it introduces latency. The database must be queried for every single visitor (user or bot). If the geospatial query is complex, Time to First Byte (TTFB) suffers, negatively impacting Core Web Vitals and, consequently, rankings.
3.1.3 The Solution: Incremental Static Regeneration (ISR)
ISR is the architectural key to the Internal Spiderweb. It allows a hybrid approach:
1. Critical Path Pre-rendering: The most important pages (e.g., top 50 cities) are generated at build time.
2. Lazy Generation: Deep pages (e.g., small suburbs) are generated on-demand when first requested by a crawler.
3. Background Revalidation: Once generated, the static HTML is cached. Next.js can be configured to revalidate this cache periodically (e.g., every 7 days).
   * Relevance to Linking: This means the complex calculation of "Who are my 50 nearest neighbors?" happens only once per revalidation cycle, not on every visit. This allows for computationally expensive, high-precision link graph generation without degrading user experience.12
3.2 React Server Components (RSC) and Payload Efficiency
In the modern Next.js App Router, the "Related" component is a Server Component. This is a critical distinction from client-side React.
* Mechanism: The logic to fetch links (connecting to PostGIS, calculating vectors) runs entirely on the server. The resulting list of 50 links is rendered into HTML <a> tags and sent to the browser.
* Benefit: The client-side JavaScript bundle does not include the heavy geospatial libraries or the data fetching logic. This keeps the page lightweight and performant, despite the high density of links. Googlebot receives a fully populated HTML document (perfect for crawling) while the user receives a fast, interactive page.14
________________
4. The "Related" Component: Algorithmic Architecture
The core of the Internal Spiderweb is the "Related" component—a logic controller responsible for programmatically selecting the most relevant internal links for any given page. To achieve the "dense mesh" required to trap crawlers, this component must aggregate links from three distinct vectors of relevance: Geospatial, Topical, and Hierarchical.
4.1 Vector 1: Geospatial Proximity (The "Nearby" Graph)
The strongest signal for local service SEO is geography. A user (and thus a crawler) interested in "Plumbing in Austin" is statistically likely to be interested in "Plumbing in Round Rock" (a neighbor) but not "Plumbing in Dallas" (distant).
4.1.1 The Mathematics of Proximity
To automate this, the system must calculate the distance between the current page's location and all other available locations in the database.
The Haversine Formula is the standard for calculating great-circle distances on a sphere:


$$d = 2r \arcsin\left(\sqrt{\sin^2\left(\frac{\Delta\phi}{2}\right) + \cos(\phi_1)\cos(\phi_2)\sin^2\left(\frac{\Delta\lambda}{2}\right)}\right)$$
Where $\phi$ is latitude, $\lambda$ is longitude, and $r$ is the earth's radius (6,371 km). While this can be implemented in JavaScript, iterating this calculation over thousands of nodes during a request is inefficient.17
4.1.2 Database-Level Optimization (PostGIS)
For scalable pSEO, the geospatial logic must be offloaded to the database. PostgreSQL with the PostGIS extension is the industry standard for this application. PostGIS utilizes R-Tree spatial indices (GiST) to perform nearest-neighbor queries in logarithmic time, rather than linear time.
The KNN Query Pattern:
Instead of calculating distance to all points and sorting (expensive), PostGIS allows for "K-Nearest Neighbors" (KNN) traversal using the <-> operator.


SQL




SELECT 
   l.city_name,
   l.slug,
   ST_Distance(l.geom, current.geom) as distance
FROM 
   locations l,
   (SELECT geom FROM locations WHERE slug = $1) as current
WHERE 
   l.slug!= $1 -- Exclude self
ORDER BY 
   l.geom <-> current.geom -- KNN operator
LIMIT 50; -- The "50 relevant paths" requirement

This query typically executes in sub-millisecond time, allowing the Next.js server to fetch the 50 closest suburbs instantly, forming the backbone of the geospatial mesh.19
4.1.3 Data Sourcing
To populate this database, reliable geodata is required. GeoNames is the primary source for pSEO, offering a free database of millions of place names with coordinates.
* Implementation: Download the US.zip (for US sites) from GeoNames, parse the tab-delimited file, and import it into the PostGIS locations table. This provides the raw nodes (cities, suburbs, CDPs) for the graph.22
4.2 Vector 2: Service Affinity (The "Topic" Graph)
While geography provides horizontal breadth, service affinity provides vertical depth. Linking "Plumber in Austin" to "Leak Detection in Austin" creates a semantic cluster around the entity "Austin."
4.2.1 Taxonomy-Based Linking
The simplest implementation relies on a predefined taxonomy structure.
* Structure:
   * Category: Plumbing
      * Service: Drain Cleaning
      * Service: Leak Detection
      * Service: Water Heater Repair
* Logic: When rendering a page for Service A, the component queries the database for all Service B entries that share the same Location ID.
4.2.2 Semantic Vector Search (Advanced)
A more sophisticated approach utilizes Vector Embeddings. By embedding service descriptions using a model like OpenAI's text-embedding-3-small and storing them in a vector database (e.g., pgvector, Pinecone), the system can find non-obvious relationships.
* Scenario: A page for "Sewer Line Replacement" might not be explicitly categorized with "Excavation Services," but a vector search would identify them as semantically close.
* Application: The "Related" component fetches the 10 most semantically similar services available in the current city, creating a mesh based on meaning rather than just category tags.8
4.3 Vector 3: Hierarchical Hubs (The "Parent" Graph)
To prevent the crawler from getting stuck in an infinite loop of small suburbs (a "spider trap" in the negative sense), the component must provide upward mobility.
* Breadcrumbs & Hubs: Links to "Plumbers in [County]" or "Plumbers in" must be included. This allows Link Equity to flow back up to the major aggregation pages, which often target high-volume "head terms".1
________________
5. Technical Implementation in Next.js
The implementation of the Spiderweb involves orchestrating the data fetching and rendering within the Next.js App Router framework. This section details the code architecture required to achieve the "50 high-speed paths" goal.
5.1 Component Architecture
The InternalSpiderweb component is designed as an asynchronous Server Component. It accepts the context of the current page (Service and Location) and returns the rendered list of links.


TypeScript




// app/components/InternalSpiderweb.tsx
import { db } from '@/lib/db'; // Assumes Prisma or similar ORM
import Link from 'next/link';

interface Props {
 serviceSlug: string;
 locationSlug: string;
}

export async function InternalSpiderweb({ serviceSlug, locationSlug }: Props) {
 // 1. Resolve current entities
 const currentLocation = await db.location.findUnique({ where: { slug: locationSlug } });
 const currentService = await db.service.findUnique({ where: { slug: serviceSlug } });

 if (!currentLocation ||!currentService) return null;

 // 2. Parallel Data Fetching
 // We fetch both geospatial neighbors and related services simultaneously
 // to minimize blocking time.
 const = await Promise.all();

 return (
   <div className="spiderweb-mesh grid grid-cols-1 md:grid-cols-2 gap-8 p-6 bg-gray-50 rounded-lg">
     <div className="geo-cluster">
       <h3 className="font-bold text-lg mb-4">
           {currentService.name} services near {currentLocation.name}
       </h3>
       <ul className="grid grid-cols-2 gap-2 text-sm">
         {nearbyLocations.map((loc: any) => (
           <li key={loc.slug}>
             <Link 
               href={`/${serviceSlug}/${loc.slug}`}
               prefetch={false} // Performance Optimization (See Section 6)
               className="hover:text-blue-600 transition-colors"
             >
               {loc.name}
             </Link>
           </li>
         ))}
       </ul>
     </div>

     <div className="topic-cluster">
       <h3 className="font-bold text-lg mb-4">
           Other services in {currentLocation.name}
       </h3>
       <ul className="grid grid-cols-2 gap-2 text-sm">
         {relatedServices.map((svc) => (
           <li key={svc.slug}>
             <Link 
               href={`/${svc.slug}/${locationSlug}`}
               prefetch={false}
               className="hover:text-blue-600 transition-colors"
             >
               {svc.name}
             </Link>
           </li>
         ))}
       </ul>
     </div>
   </div>
 );
}

5.2 The "Trap" Logic: Creating Cycles
For the spiderweb to function as a trap, the links must form Cycles, not dead ends.
* Reciprocity: If "Austin" links to "Round Rock" because it is a near neighbor, "Round Rock" will essentially link back to "Austin" because "Austin" is also its near neighbor. This bidirectional linking creates a robust feedback loop for crawlers.
* The Hub-Spoke-Rim Model:
   * Hub: The County or Major City page.
   * Spoke: Links radiating out to suburbs.
   * Rim: The programmatically generated links between suburbs (e.g., Round Rock <-> Pflugerville).
   * Effect: A crawler can traverse the "Rim" (the deep programmatic pages) indefinitely without needing to return to the Hub, yet the Hub constantly feeds authority into the Rim. This ensures that deep pages support each other, maintaining indexation even if the Hub link is temporarily lost.5
5.3 Preventing Orphan Pages via Auditing
A common risk in pSEO is the "Island Problem," where a group of pages is generated but, due to data anomalies (e.g., a city geographically isolated from others), rarely appears in the "Nearest Neighbor" lists of other pages.
To mitigate this, a Build-Time Orphan Detection Script is required.
5.3.1 Script Logic
1. Generate Sitemap: Use next-sitemap to produce the full list of intended URLs.
2. Crawl Build Output: Traverse the .next/server/app directory (or use a local crawler like globby) to parse the HTML and extract all internal href attributes.
3. Graph Analysis: Construct a directed graph in memory where every URL is a node and every href is an edge.
4. Identify Orphans: Filter for nodes with an In-Degree of 0 (no incoming links).


JavaScript




// scripts/audit-orphans.mjs
import fs from 'fs';
import globby from 'globby';
import cheerio from 'cheerio';

async function audit() {
 const pages = await globby(['.next/server/app/**/*.html']);
 const adjacencyList = new Map();
 const allUrls = new Set();

 // 1. Build the Graph
 for (const pagePath of pages) {
   const html = fs.readFileSync(pagePath, 'utf8');
   const $ = cheerio.load(html);
   const sourceUrl = extractUrlFromPath(pagePath); // Helper fn
   allUrls.add(sourceUrl);

   $('a').each((i, link) => {
     const href = $(link).attr('href');
     if (href && href.startsWith('/')) {
        // Record the edge
        if (!adjacencyList.has(href)) adjacencyList.set(href, 0);
        adjacencyList.set(href, adjacencyList.get(href) + 1);
     }
   });
 }

 // 2. Detect Orphans
 const orphans =;
 for (const url of allUrls) {
   if (!adjacencyList.has(url) |

| adjacencyList.get(url) === 0) {
     orphans.push(url);
   }
 }

 console.log(`Found ${orphans.length} orphan pages.`);
 // 3. Fail build if threshold exceeded
 if (orphans.length > 0) process.exit(1);
}

audit();

This script acts as a CI/CD gatekeeper, preventing the deployment of a mesh with holes.25
________________
6. Performance Engineering & User Experience
Injecting 50-100 links into the DOM of every page carries significant performance implications. If not managed, this can degrade the user experience, increase bandwidth costs, and harm Core Web Vitals (specifically Interaction to Next Paint - INP).
6.1 The Cost of next/link Prefetching
By default, the Next.js <Link> component prefetches the code and data for the linked route when the link enters the viewport.
* The Scenario: A user loads "Plumber in Austin." The "Related" component renders 50 links to nearby suburbs. As the user scrolls to the footer, all 50 links enter the viewport.
* The Consequence: The browser attempts to fire 50 simultaneous network requests to fetch the JSON data for those 50 pages. This creates network congestion, spikes CPU usage, and makes the site unresponsive on mobile devices.
* The Fix: Explicitly set prefetch={false} on all programmatic links in the Spiderweb.
   * Rationale: These links are primarily for the crawler (which parses HTML and queues URLs) and secondarily for the user. We do not need instant client-side transition performance for these lateral links. The crawler does not execute the prefetch JavaScript; it simply follows the href.28
6.2 DOM Size and Hydration
Google recommends keeping DOM nodes under 1,500 per page. A dense spiderweb can push this limit.
* Server Component Advantage: As established, using Server Components means the hydration JSON (the data used to make React interactive) does not need to include the props for these 50 links if they are static HTML anchors. This significantly reduces the hydration weight compared to Next.js Page Router or standard React.
* Visual Stability: Ensure the "Related" component has a fixed height or is rendered in a container that prevents Layout Shift (CLS) when the data loads (if using streaming).
6.3 Visualizing the Graph
To ensure the "Dense Mesh" promise is met, visualization tools are essential.
* Force-Directed Graphs: Using tools like Gephi or Sitebulb, one can visualize the crawl data. A healthy pSEO spiderweb should look like a dense "ball of wool," indicating high connectivity (Cluster Coefficient). A poorly interconnected site will look like a "dandelion" (strong center, weak disconnected periphery).
* Integration: Regular audits using these visualization tools confirm that the geospatial algorithm is effectively bridging gaps between clusters.30
________________
7. Schema Markup: The Semantic Layer
The Internal Spiderweb creates the pathways for the crawler. Schema Markup provides the map legend. For the crawler to understand why "Austin" is linked to "Round Rock," we must use structured data.
7.1 LocalBusiness and ServiceArea
For every programmatic page, inject JSON-LD schema that explicitly defines the service area and the relationship to the parent organization.


JSON




{
 "@context": "https://schema.org",
 "@type": "Service",
 "serviceType": "Plumbing",
 "provider": {
   "@type": "LocalBusiness",
   "name": "Master Plumbers Austin",
   "areaServed":
 },
 "url": "https://example.com/plumber/austin"
}

By explicitly listing the areaServed using the same data that generates the visual links, we reinforce the geospatial cluster in language the search engine natively understands (JSON-LD). This alignment between the visual link graph and the semantic data graph provides a powerful ranking signal.32
________________
8. Conclusion: The Mesh Advantage
The "Internal Spiderweb" represents the maturation of Programmatic SEO. It acknowledges that in a world of infinite content, connectivity is the currency of relevance. By implementing this architecture in Next.js, we leverage specific technical advantages—ISR for infinite scale, Server Components for performance, and PostGIS for algorithmic precision—that are simply inaccessible to competitors relying on generic CMS plugins.
The result is a crawl trap in the most effective sense: a domain where Googlebot enters via a single node and is immediately presented with a high-speed, relevant, and mathematically optimized network of pathways. It does not just index the page it lands on; it is compelled, by the very topology of the site, to index the entire domain, rapidly establishing authority across thousands of long-tail keywords. This is the difference between a site that has content, and a site that ranks.
________________
9. Appendix: Implementation Checklist
Component
	Technology
	Requirement
	Rendering
	Next.js ISR
	Configure revalidate time (e.g., 604800s) to refresh links weekly.
	Database
	PostgreSQL + PostGIS
	Enable postgis extension. Index geom column with GiST.
	Query
	SQL
	Use <-> (KNN) operator for neighbor discovery. Limit 50.
	Component
	React Server Component
	Fetch data on server. Use <Link prefetch={false}>.
	Audit
	Node.js Script
	Compare sitemap.xml vs. crawled internal links to find orphans.
	Schema
	JSON-LD
	Inject areaServed matching the link graph.
	________________
10. Deep Dive: Geospatial Algorithms and Data Structures
To fully appreciate the robustness of the Internal Spiderweb, one must explore the underlying data structures that make "finding 50 neighbors for 50,000 cities" computationally feasible.
10.1 The R-Tree Index
PostGIS uses R-Trees (Rectangle Trees) for spatial indexing. An R-Tree groups nearby objects and represents them with their minimum bounding rectangle in the next higher level of the tree.
* Mechanism: When the "Related" component queries for "Plumbers near Austin," the database does not scan every city in the US. It traverses the R-Tree, discarding vast branches of the tree (e.g., the entire West Coast) that do not intersect with the Austin bounding box.
* Implication: This allows the link generation query to run in $O(\log n)$ time. Without this, the query would be $O(n)$, and generating the site would grind to a halt. This is why a simple SQL database without spatial extensions is insufficient for high-scale pSEO.19
10.2 Handling "Islands" in the Graph
A common failure mode in geospatial linking is the "Island Effect," where rural towns are too far from any major city to appear in a "Nearest 50" list restricted by a tight radius.
* Adaptive Radius Algorithm: The query logic in the InternalSpiderweb component should be adaptive.
   * Attempt 1: Find neighbors within 20km.
   * Check: If count < 10, expand radius to 50km.
   * Check: If still < 10, expand to 100km or query by "Same County."
* Code Implementation: This logic can be encapsulated in a database stored procedure or handled in the Next.js service layer, ensuring that even the most remote nodes maintain connectivity to the main mesh.17
10.3 The "Spider Trap" Risk (Infinite Loops)
While we want to trap the crawler, we must avoid creating "black holes"—infinite sequences of unique URLs that provide no value (e.g., infinite calendar generations or parameter permutations).
* Prevention: The Spiderweb relies strictly on canonical URLs defined in the database (/plumber/austin). It avoids query parameters (/plumber?loc=austin) for internal linking, as these are harder for crawlers to de-duplicate.
* Robots.txt: Explicitly Disallow patterns that might generate low-value variations, protecting the crawl budget for the high-value mesh nodes.16
________________
11. Advanced Semantic Linking: Vector Embeddings
While geospatial linking is deterministic, topical linking is probabilistic. To achieve the highest quality "Service-Lateral" links, we move beyond simple taxonomy.
11.1 The Semantic Gap
A taxonomy might group "Drain Cleaning" and "Sewer Repair" together. But it might miss the connection between "Sewer Repair" and "Biohazard Cleanup" (which might be in a different category). A user—and a sophisticated crawler like Google—understands this relationship.
11.2 Vector Implementation
1. Embedding: During the content generation phase, pass the service description through an embedding model (e.g., OpenAI text-embedding-3-small).
2. Storage: Store the resulting 1536-dimensional vector in a vector column in Postgres (using pgvector).
3. Querying:
SQL
SELECT name, slug 
FROM services 
ORDER BY embedding <=> (SELECT embedding FROM services WHERE slug = $current) 
LIMIT 10;

4. Result: The "Related" component now suggests links based on conceptual similarity. If "Water Heater Repair" is semantically close to "Gas Line Installation" (due to both involving piping and gas), the link is made. This creates a "Semantic Mesh" overlaid on top of the "Geospatial Mesh," doubling the density and relevance of the trap.8
________________
12. Final Architecture Review
The "Internal Spiderweb" is a holistic system. It is not a single plugin or a script, but a convergence of:
   1. Data Engineering: High-quality geospatial and vector data.
   2. Database Optimization: PostGIS R-Trees and pgvector indices.
   3. Application Architecture: Next.js ISR and Server Components for performant rendering.
   4. Graph Theory: Small-world network topology for optimal crawl traversal.
   5. Semantic SEO: Schema markup and vector relevance for machine understanding.
By implementing this system, a pSEO campaign moves from a "spray and pray" approach to a precision-engineered dominance of the search index. The crawler is not merely invited to the site; it is captured by the utility and density of the network, ensuring that the programmatic content—no matter how vast—is discovered, indexed, and ranked.